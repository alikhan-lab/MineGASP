{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Похоже, вы хотите, чтобы я представил вам **два приложения на C++**, которые используют **только библиотеку `Pytorch`** (или, скорее всего, **обёртку PyTorch C++ API**, известную как **LibTorch**).\n\nPyTorch — это фреймворк глубокого обучения, и его использование через C++ (LibTorch) обычно требуется для продакшн-развёртывания моделей, где Python нежелателен или недоступен.\n\nВот два примера, демонстрирующих базовые возможности LibTorch:\n\n---\n\n## Пример 1: Загрузка и инференс (Inference) предобученной модели (Использование `torch::jit::load`)\n\nЭтот пример показывает, как загрузить модель, сохранённую в формате TorchScript (обычно с расширением `.pt` или `.pth`), и выполнить прямой проход (инференс).\n\n**Предварительное условие:** Вам нужно скомпилировать модель в PyTorch (Python) в формат TorchScript, используя `torch.jit.trace` или `torch.jit.script`.\n\n### Код на C++ (LibTorch)\n\n```cpp\n#include \u003ctorch/torch.h\u003e\n#include \u003ciostream\u003e\n#include \u003cmemory\u003e\n\nvoid run_inference_example() {\n    std::cout \u003c\u003c \"--- Пример 1: Загрузка и инференс модели ---\" \u003c\u003c std::endl;\n\n    try {\n        // 1. Загрузка модели (предполагается, что 'model.pt' - это TorchScript-модель)\n        std::shared_ptr\u003ctorch::jit::script::Module\u003e module = \n            torch::jit::load(\"model.pt\");\n\n        if (!module) {\n            std::cerr \u003c\u003c \"Ошибка: Не удалось загрузить модель 'model.pt'. \"\n                      \u003c\u003c \"Убедитесь, что файл существует и является валидной TorchScript моделью.\" \u003c\u003c std::endl;\n            return;\n        }\n        \n        module-\u003eeval(); // Устанавливаем модуль в режим инференса\n\n        // 2. Подготовка входных данных (например, тензор 1x3x224x224, как для CNN)\n        torch::Tensor input = torch::randn({1, 3, 224, 224});\n        std::vector\u003ctorch::jit::IValue\u003e inputs;\n        inputs.push_back(input);\n\n        // 3. Выполнение инференса\n        torch::Tensor output = module-\u003eforward(inputs).toTensor();\n\n        // 4. Обработка результатов\n        std::cout \u003c\u003c \"Модель успешно загружена и выполнила инференс.\" \u003c\u003c std::endl;\n        std::cout \u003c\u003c \"Форма выходного тензора: \" \u003c\u003c output.sizes() \u003c\u003c std::endl;\n        \n        // Вывод первого элемента результата для проверки\n        std::cout \u003c\u003c \"Первый элемент выхода: \" \u003c\u003c output.accessor\u003cfloat, 2\u003e()[0][0] \u003c\u003c std::endl;\n\n    } catch (const c10::Error& e) {\n        std::cerr \u003c\u003c \"Ошибка LibTorch при выполнении инференса: \" \u003c\u003c e.what() \u003c\u003c std::endl;\n    }\n}\n\nint main() {\n    // Проверка доступности LibTorch\n    if (torch::has_cuda()) {\n        std::cout \u003c\u003c \"CUDA доступна. Устройство: \" \u003c\u003c torch::cuda::get_device_name(0) \u003c\u003c std::endl;\n    } else {\n        std::cout \u003c\u003c \"CUDA недоступна. Используется CPU.\" \u003c\u003c std::endl;\n    }\n    \n    run_inference_example();\n    return 0;\n}\n// ПРИМЕЧАНИЕ: Для компиляции нужен настроенный LibTorch и компилятор C++17 или выше.\n```\n\n---\n\n## Пример 2: Создание, обучение (минимальное) и сохранение простого тензора (Базовые операции тензоров)\n\nХотя LibTorch чаще используется для инференса, он полностью поддерживает создание тензоров, математические операции и базовое обучение (если вы не используете C++ для сложной оптимизации градиентов). Этот пример фокусируется на создании и манипуляции тензорами.\n\n### Код на C++ (LibTorch)\n\n```cpp\n#include \u003ctorch/torch.h\u003e\n#include \u003ciostream\u003e\n\nvoid tensor_manipulation_example() {\n    std::cout \u003c\u003c \"\\n--- Пример 2: Создание и манипуляция тензорами ---\" \u003c\u003c std::endl;\n\n    // 1. Создание тензора с данными\n    torch::Tensor tensor_a = torch::tensor({{1.0, 2.0}, {3.0, 4.0}});\n    std::cout \u003c\u003c \"Тензор A:\\n\" \u003c\u003c tensor_a \u003c\u003c std::endl;\n\n    // 2. Создание тензора случайных чисел\n    torch::Tensor tensor_b = torch::rand({2, 2});\n    std::cout \u003c\u003c \"Тензор B (случайный):\\n\" \u003c\u003c tensor_b \u003c\u003c std::endl;\n\n    // 3. Математическая операция (Сложение)\n    torch::Tensor tensor_sum = tensor_a + tensor_b;\n    std::cout \u003c\u003c \"Тензор A + B:\\n\" \u003c\u003c tensor_sum \u003c\u003c std::endl;\n\n    // 4. Преобразование типов и изменение формы (Reshape)\n    torch::Tensor reshaped_tensor = tensor_a.to(torch::kInt32).view({4});\n    std::cout \u003c\u003c \"Тензор A (измененный тип и форма):\\n\" \u003c\u003c reshaped_tensor \u003c\u003c std::endl;\n\n    // 5. Сохранение простого тензора (для демонстрации сериализации)\n    try {\n        torch::save(reshaped_tensor, \"simple_tensor.pt\");\n        std::cout \u003c\u003c \"Тензор сохранен в simple_tensor.pt\" \u003c\u003c std::endl;\n\n        // Загрузка для проверки\n        torch::Tensor loaded_tensor;\n        torch::load(loaded_tensor, \"simple_tensor.pt\");\n        std::cout \u003c\u003c \"Загруженный тензор:\\n\" \u003c\u003c loaded_tensor \u003c\u003c std::endl;\n\n    } catch (const std::exception& e) {\n        std::cerr \u003c\u003c \"Ошибка при сохранении/загрузке: \" \u003c\u003c e.what() \u003c\u003c std::endl;\n    }\n}\n\nint main() {\n    tensor_manipulation_example();\n    return 0;\n}\n```\n\n---\n\n## Как это запустить (Обзор компиляции)\n\nДля компиляции любого из этих примеров вам необходимо:\n\n1.  **Установить LibTorch:** Скачать с официального сайта PyTorch (убедитесь, что версия соответствует вашему компилятору и ОС).\n2.  **Настроить компилятор:** Использовать CMake, `g++` (или `cl.exe` для MSVC) и указать пути к заголовочным файлам (`include`) и библиотекам (`lib`) LibTorch.\n\n### Пример команды компиляции (GCC/Linux, без CMake, только для демонстрации):\n\n```bash\n# Предположим, что LIBTORCH_DIR указывает на папку, куда вы распаковали LibTorch\ng++ -std=c++17 -o app_torch app.cpp \\\n    -I${LIBTORCH_DIR}/include \\\n    -I${LIBTORCH_DIR}/include/torch/csrc/api/include \\\n    -L${LIBTORCH_DIR}/lib \\\n    -ltorch -ltorch_cpu -pthread -ldl -Wl,--no-as-needed\n```"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "index": 0
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 111,
    "candidatesTokenCount": 1722,
    "totalTokenCount": 1833,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 111
      }
    ]
  },
  "modelVersion": "gemini-2.5-flash-lite-preview-09-2025",
  "responseId": "pBxeadSUH8TPnsEP-cO02Ag"
}
